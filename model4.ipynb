{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\study\\4th year sbme\\1st Term\\Deep Learning\\GANs-Super-Resolution\\.venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESRGAN:\n",
    "    \"\"\"\n",
    "    Implementation of ESRGAN following the paper:\n",
    "    'ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks'\n",
    "    For grayscale medical images.\n",
    "    \"\"\"\n",
    "    def __init__(self, scale_factor=4):\n",
    "        self.scale_factor = scale_factor\n",
    "        self.generator = None\n",
    "        self.discriminator = None\n",
    "        self.vgg = None\n",
    "        self.build_models()\n",
    "        \n",
    "    def _residual_dense_block(self, x, features=64):\n",
    "        \"\"\"Residual Dense Block\"\"\"\n",
    "        concat_features = []\n",
    "        input_features = x\n",
    "        \n",
    "        for i in range(5):\n",
    "            if concat_features:\n",
    "                x = layers.Concatenate()(concat_features + [x])\n",
    "            x = layers.Conv2D(features, (3, 3), padding='same')(x)\n",
    "            x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "            concat_features.append(x)\n",
    "            \n",
    "        x = layers.Concatenate()(concat_features)\n",
    "        x = layers.Conv2D(features, (1, 1), padding='same')(x)\n",
    "        \n",
    "        # Local residual learning\n",
    "        return layers.Add()([input_features, x * 0.2])\n",
    "    \n",
    "    def _rrdb_block(self, x, features=64):\n",
    "        \"\"\"Residual in Residual Dense Block\"\"\"\n",
    "        input_features = x\n",
    "        \n",
    "        for _ in range(3):\n",
    "            x = self._residual_dense_block(x, features)\n",
    "            \n",
    "        # Residual scaling\n",
    "        return layers.Add()([input_features, x * 0.2])\n",
    "    \n",
    "    def build_models(self):\n",
    "        # Generator (Modified for Grayscale)\n",
    "        lr_input = Input(shape=(None, None, 1))  # Grayscale input (single channel)\n",
    "        \n",
    "        # First conv\n",
    "        x = layers.Conv2D(64, (3, 3), padding='same')(lr_input)\n",
    "        initial_feature = x\n",
    "        \n",
    "        # RRDB blocks (23 blocks as in paper)\n",
    "        for _ in range(23):\n",
    "            x = self._rrdb_block(x)\n",
    "            \n",
    "        # Global feature fusion\n",
    "        x = layers.Conv2D(64, (3, 3), padding='same')(x)\n",
    "        trunk = layers.Add()([initial_feature, x])\n",
    "        \n",
    "        # Upsampling blocks (4x)\n",
    "        for _ in range(2):  # Two blocks for 4x upsampling\n",
    "            x = layers.Conv2D(256, (3, 3), padding='same')(trunk)\n",
    "            x = tf.nn.depth_to_space(x, 2)  # Pixel shuffle\n",
    "            x = layers.LeakyReLU(0.2)(x)\n",
    "            trunk = x\n",
    "        \n",
    "        # Final conv\n",
    "        sr_output = layers.Conv2D(1, (3, 3), padding='same', activation='tanh')(trunk)  # Single channel output\n",
    "        \n",
    "        self.generator = Model(lr_input, sr_output, name='generator')\n",
    "        \n",
    "        # Discriminator (Modified for Grayscale)\n",
    "        def d_block(x, filters, strides=1, bn=True):\n",
    "            x = layers.Conv2D(filters, (3, 3), strides=strides, padding='same')(x)\n",
    "            if bn:\n",
    "                x = layers.BatchNormalization()(x)\n",
    "            return layers.LeakyReLU(alpha=0.2)(x)\n",
    "        \n",
    "        d_input = Input(shape=(None, None, 1))  # Grayscale input\n",
    "        \n",
    "        # Series of Conv + LeakyReLU + BN\n",
    "        features = [64, 64, 128, 128, 256, 256, 512, 512]\n",
    "        x = d_input\n",
    "        \n",
    "        for idx, f in enumerate(features):\n",
    "            x = d_block(x, f, strides=2 if idx % 2 == 1 else 1)\n",
    "        \n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dense(1024)(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = layers.Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "        self.discriminator = Model(d_input, x, name='discriminator')\n",
    "        \n",
    "        # VGG feature extractor for perceptual loss\n",
    "        vgg = VGG19(include_top=False, weights='imagenet', input_shape=(None, None, 3))  # VGG expects 3 channels, but we'll use grayscale\n",
    "        self.vgg = Model(inputs=vgg.input,\n",
    "                        outputs=vgg.get_layer('block5_conv4').output,\n",
    "                        name='vgg')\n",
    "        self.vgg.trainable = False\n",
    "        \n",
    "    def compile(self, \n",
    "                gen_lr=1e-4, \n",
    "                disc_lr=1e-4,\n",
    "                content_weight=1.0,\n",
    "                perceptual_weight=1.0,\n",
    "                adversarial_weight=0.1):\n",
    "        \n",
    "        self.gen_optimizer = tf.keras.optimizers.Adam(gen_lr, beta_1=0.9, beta_2=0.99)\n",
    "        self.disc_optimizer = tf.keras.optimizers.Adam(disc_lr, beta_1=0.9, beta_2=0.99)\n",
    "        \n",
    "        self.content_weight = content_weight\n",
    "        self.perceptual_weight = perceptual_weight\n",
    "        self.adversarial_weight = adversarial_weight\n",
    "        \n",
    "    @tf.function\n",
    "    def train_step(self, lr_images, hr_images):\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            # Generate fake images\n",
    "            sr_images = self.generator(lr_images, training=True)\n",
    "            \n",
    "            # Convert HR and SR images from grayscale (1 channel) to RGB (3 channels)\n",
    "            hr_images_rgb = tf.image.grayscale_to_rgb(hr_images)\n",
    "            sr_images_rgb = tf.image.grayscale_to_rgb(sr_images)\n",
    "\n",
    "            # Extract features using VGG (for perceptual loss)\n",
    "            hr_features = self.vgg(hr_images_rgb)\n",
    "            sr_features = self.vgg(sr_images_rgb)\n",
    "\n",
    "            # Discriminator outputs\n",
    "            real_output = self.discriminator(hr_images, training=True)\n",
    "            fake_output = self.discriminator(sr_images, training=True)\n",
    "            \n",
    "            # Content loss (L1 loss as per paper)\n",
    "            content_loss = tf.reduce_mean(tf.abs(hr_images - sr_images))\n",
    "            \n",
    "            # Perceptual loss\n",
    "            hr_features = self.vgg(hr_images)\n",
    "            sr_features = self.vgg(sr_images)\n",
    "            perceptual_loss = tf.reduce_mean(tf.abs(hr_features - sr_features))\n",
    "            \n",
    "            # Relativistic average GAN loss\n",
    "            real_logits = real_output - tf.reduce_mean(fake_output)\n",
    "            fake_logits = fake_output - tf.reduce_mean(real_output)\n",
    "            \n",
    "            disc_loss = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=tf.ones_like(real_logits), logits=real_logits\n",
    "                ) +\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=tf.zeros_like(fake_logits), logits=fake_logits\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            gen_loss = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=tf.ones_like(fake_logits), logits=fake_logits\n",
    "                ) +\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=tf.zeros_like(real_logits), logits=real_logits\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Total generator loss\n",
    "            total_gen_loss = (\n",
    "                self.content_weight * content_loss +\n",
    "                self.perceptual_weight * perceptual_loss +\n",
    "                self.adversarial_weight * gen_loss\n",
    "            )\n",
    "            \n",
    "        # Compute gradients\n",
    "        gen_gradients = gen_tape.gradient(\n",
    "            total_gen_loss, self.generator.trainable_variables\n",
    "        )\n",
    "        disc_gradients = disc_tape.gradient(\n",
    "            disc_loss, self.discriminator.trainable_variables\n",
    "        )\n",
    "        \n",
    "        # Apply gradients\n",
    "        self.gen_optimizer.apply_gradients(\n",
    "            zip(gen_gradients, self.generator.trainable_variables)\n",
    "        )\n",
    "        self.disc_optimizer.apply_gradients(\n",
    "            zip(disc_gradients, self.discriminator.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'content_loss': content_loss,\n",
    "            'perceptual_loss': perceptual_loss,\n",
    "            'gen_loss': gen_loss,\n",
    "            'disc_loss': disc_loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\study\\4th year sbme\\1st Term\\Deep Learning\\GANs-Super-Resolution\\.venv\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\study\\4th year sbme\\1st Term\\Deep Learning\\GANs-Super-Resolution\\.venv\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, image_dir, batch_size=16, hr_size=128, scale_factor=4):\n",
    "        self.image_dir = image_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.hr_size = hr_size\n",
    "        self.lr_size = hr_size // scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        \n",
    "        self.dataset = self._create_dataset()\n",
    "    \n",
    "    def _load_and_process(self, path):\n",
    "        # Load image\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_png(img, channels=1)  # Read as grayscale\n",
    "        img = tf.cast(img, tf.float32) / 127.5 - 1  # Normalize to [-1, 1]\n",
    "        \n",
    "        # Random crop\n",
    "        img = tf.image.random_crop(img, [self.hr_size, self.hr_size, 1])\n",
    "        \n",
    "        # Create low-res version\n",
    "        lr_img = tf.image.resize(img, [self.lr_size, self.lr_size],\n",
    "                               method='bicubic')\n",
    "        \n",
    "        return lr_img, img\n",
    "    \n",
    "    def _create_dataset(self):\n",
    "        # Get image paths\n",
    "        image_paths = tf.data.Dataset.list_files(str(self.image_dir + '/*'))\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = (image_paths\n",
    "                  .map(self._load_and_process, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                  .batch(self.batch_size)\n",
    "                  .prefetch(tf.data.AUTOTUNE))\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "# Initialize model\n",
    "model = ESRGAN(scale_factor=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile with custom loss weights if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    gen_lr=1e-4, \n",
    "    disc_lr=1e-4,\n",
    "    content_weight=1.0, \n",
    "    perceptual_weight=1.0,\n",
    "    adversarial_weight=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(image_dir='Dataset/images', batch_size=16, hr_size=128, scale_factor=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Kimo Store\\AppData\\Local\\Temp\\ipykernel_2628\\2687994062.py\", line 136, in train_step  *\n        hr_features = self.vgg(hr_images)\n    File \"d:\\study\\4th year sbme\\1st Term\\Deep Learning\\GANs-Super-Resolution\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"d:\\study\\4th year sbme\\1st Term\\Deep Learning\\GANs-Super-Resolution\\.venv\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 280, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'vgg' (type Functional).\n    \n    Input 0 of layer \"block1_conv1\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (16, 128, 128, 1)\n    \n    Call arguments received by layer 'vgg' (type Functional):\n      • inputs=tf.Tensor(shape=(16, 128, 128, 1), dtype=float32)\n      • training=None\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lr_images, hr_images \u001b[38;5;129;01min\u001b[39;00m data_loader\u001b[38;5;241m.\u001b[39mdataset:\n\u001b[1;32m----> 2\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhr_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(losses)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Just one batch for demonstration\u001b[39;00m\n",
      "File \u001b[1;32md:\\study\\4th year sbme\\1st Term\\Deep Learning\\GANs-Super-Resolution\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\KIMOST~1\\AppData\\Local\\Temp\\__autograph_generated_filezxrncgmd.py:19\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[1;34m(self, lr_images, hr_images)\u001b[0m\n\u001b[0;32m     17\u001b[0m fake_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdiscriminator, (ag__\u001b[38;5;241m.\u001b[39mld(sr_images),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope)\n\u001b[0;32m     18\u001b[0m content_loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mreduce_mean, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mabs, (ag__\u001b[38;5;241m.\u001b[39mld(hr_images) \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(sr_images),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 19\u001b[0m hr_features \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvgg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhr_images\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m sr_features \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mvgg, (ag__\u001b[38;5;241m.\u001b[39mld(sr_images),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     21\u001b[0m perceptual_loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mreduce_mean, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mabs, (ag__\u001b[38;5;241m.\u001b[39mld(hr_features) \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(sr_features),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32md:\\study\\4th year sbme\\1st Term\\Deep Learning\\GANs-Super-Resolution\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\study\\4th year sbme\\1st Term\\Deep Learning\\GANs-Super-Resolution\\.venv\\lib\\site-packages\\keras\\src\\engine\\input_spec.py:280\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    275\u001b[0m             value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape_as_list[\u001b[38;5;28mint\u001b[39m(axis)] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    277\u001b[0m             value,\n\u001b[0;32m    278\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    279\u001b[0m         }:\n\u001b[1;32m--> 280\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    282\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    285\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplay_shape(x\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m             )\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Kimo Store\\AppData\\Local\\Temp\\ipykernel_2628\\2687994062.py\", line 136, in train_step  *\n        hr_features = self.vgg(hr_images)\n    File \"d:\\study\\4th year sbme\\1st Term\\Deep Learning\\GANs-Super-Resolution\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"d:\\study\\4th year sbme\\1st Term\\Deep Learning\\GANs-Super-Resolution\\.venv\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 280, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'vgg' (type Functional).\n    \n    Input 0 of layer \"block1_conv1\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (16, 128, 128, 1)\n    \n    Call arguments received by layer 'vgg' (type Functional):\n      • inputs=tf.Tensor(shape=(16, 128, 128, 1), dtype=float32)\n      • training=None\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "for lr_images, hr_images in data_loader.dataset:\n",
    "    losses = model.train_step(lr_images, hr_images)\n",
    "    print(losses)\n",
    "    break  # Just one batch for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses['content_loss'], label='Content Loss')\n",
    "plt.plot(losses['perceptual_loss'], label='Perceptual Loss')\n",
    "plt.plot(losses['gen_loss'], label='Generator Loss')\n",
    "plt.plot(losses['disc_loss'], label='Discriminator Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the generated images and original high-res images and low-res images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "for i in range(4):\n",
    "    plt.subplot(4, 3, i*3 + 1)\n",
    "    plt.imshow(tf.squeeze(lr_images[i]), cmap='gray')\n",
    "    plt.title('Low-res')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(4, 3, i*3 + 2)\n",
    "    plt.imshow(tf.squeeze(hr_images[i]), cmap='gray')\n",
    "    plt.title('High-res')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    sr_images = model.generator(lr_images, training=False)\n",
    "    plt.subplot(4, 3, i*3 + 3)\n",
    "    plt.imshow(tf.squeeze(sr_images[i]), cmap='gray')\n",
    "    plt.title('Super-res')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
